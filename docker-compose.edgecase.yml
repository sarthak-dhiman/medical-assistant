services:
  # 1. Message Broker
  redis-edgecase:
    image: redis:alpine
    container_name: redis-edgecase
    ports:
      - "6380:6379"
    networks:
      - edgecase-network

  # 2. Backend API
  backend-edgecase:
    build:
      context: .
      dockerfile: web_app/backend/Dockerfile
    container_name: backend-edgecase
    ports:
      - "8001:8000"
    command: uvicorn web_app.backend.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - ./saved_models:/app/saved_models # Mount models (Hot-load + No Copy)
      - ./web_app/backend:/app/web_app/backend # Dev code sync
      - ./segformer_utils.py:/app/segformer_utils.py # Mount root files
      - ./inference_pytorch.py:/app/inference_pytorch.py
      - ./inference_new_models.py:/app/inference_new_models.py
      - ./vis_utils.py:/app/vis_utils.py
    environment:
      - REDIS_URL=redis://redis-edgecase:6379/0
      # Security: Allow frontend access
      - ALLOWED_ORIGINS=http://localhost:5174
      # GPU Support Enabled
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - TZ=${TZ:-UTC}
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    depends_on:
      - redis-edgecase
    networks:
      - edgecase-network

  # 3. AI Worker
  worker-edgecase:
    build:
      context: .
      dockerfile: web_app/backend/Dockerfile
    container_name: worker-edgecase
    command: celery -A web_app.backend.celery_app worker --loglevel=info --concurrency=1
    volumes:
      - ./saved_models:/app/saved_models
      - ./segformer_utils.py:/app/segformer_utils.py
      - ./inference_pytorch.py:/app/inference_pytorch.py
      - ./inference_new_models.py:/app/inference_new_models.py
      - ./vis_utils.py:/app/vis_utils.py
      - edgecase-hf-cache:/root/.cache/huggingface
    environment:
      - REDIS_URL=redis://redis-edgecase:6379/0
      # GPU Support Enabled
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OMP_NUM_THREADS=4
      - TZ=${TZ:-UTC}
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    depends_on:
      - redis-edgecase
    networks:
      - edgecase-network

  # 4. Monitoring (Flower)
  flower-edgecase:
    build:
      context: .
      dockerfile: web_app/backend/Dockerfile
    container_name: flower-edgecase
    command: celery -A web_app.backend.celery_app flower --port=5555 --basic_auth=admin:${FLOWER_PASSWORD:-changeme123}
    ports:
      - "5556:5555"
    environment:
      - REDIS_URL=redis://redis-edgecase:6379/0
      - FLOWER_BASIC_AUTH=admin:${FLOWER_PASSWORD:-changeme123}
      - TZ=${TZ:-UTC}
    depends_on:
      - redis-edgecase
    networks:
      - edgecase-network

  # 5. GPU Monitor (Sidecar)
  gpu-monitor-edgecase:
    build:
      context: .
      dockerfile: web_app/backend/Dockerfile
    container_name: gpu-monitor-edgecase
    command: python web_app/backend/gpu_monitor.py
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - TZ=${TZ:-UTC}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - edgecase-network
    restart: unless-stopped

  # 6. Frontend
  frontend-edgecase:
    build:
      context: .
      dockerfile: web_app/frontend/Dockerfile
    container_name: frontend-edgecase
    ports:
      - "5174:80" # Map host 5174 to container 80 (Nginx) - shifted by 1 from default 5173
    volumes:
      # Override nginx config to point to backend-edgecase
      - ./web_app/frontend/nginx.edgecase.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      backend-edgecase:
        condition: service_healthy
    networks:
      - edgecase-network

networks:
  edgecase-network:
    driver: bridge

volumes:
  edgecase-redis-data:
  edgecase-hf-cache:
