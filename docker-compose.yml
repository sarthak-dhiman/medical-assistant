services:
  # 1. Message Broker
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    networks:
      - app-network

  # 2. Backend API
  backend:
    build:
      context: .
      dockerfile: web_app/backend/Dockerfile
    image: diseaseprediction-backend:latest
    ports:
      - "8000:8000"
    command: uvicorn web_app.backend.main:app --host 0.0.0.0 --port 8000
    volumes:
      - ./saved_models:/app/saved_models # Mount models (Hot-load + No Copy)
      - ./saved_models_onnx:/app/saved_models_onnx # ONNX models (Hot-load)
      - ./external:/app/external # Mount external repos (YOLOv7)
    environment:
      - REDIS_URL=redis://redis:6379/0
      - ALLOWED_ORIGINS=http://localhost:5173
      - ALLOW_ALL_ORIGINS=True
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - ENABLE_LLM_EXPLANATIONS=${ENABLE_LLM_EXPLANATIONS}

      # GPU Support Enabled
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - TZ=${TZ:-UTC}
      # Fix for ONNXRuntime missing libnvrtc.so and libcudnn:
      - LD_LIBRARY_PATH=/opt/conda/lib/python3.10/site-packages/torch/lib:$LD_LIBRARY_PATH
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    restart: no
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 45s
      timeout: 15s
      retries: 5
      start_period: 120s
    depends_on:
      - redis
    networks:
      - app-network

  # 3a. AI Worker - Heavy (Segmentation, Body Jaundice, Skin)
  worker-heavy:
    image: diseaseprediction-backend:latest
    restart: no
    depends_on:
      redis:
        condition: service_started
      backend:
        condition: service_started
    command: celery -A web_app.backend.celery_app worker --loglevel=info --concurrency=1 -Q q_heavy_cv,celery
    volumes:
      - ./saved_models:/app/saved_models
      - ./saved_models_onnx:/app/saved_models_onnx # ONNX models
      - ./external:/app/external
      - hf_cache:/root/.cache/huggingface
    environment:
      - REDIS_URL=redis://redis:6379/0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - ENABLE_LLM_EXPLANATIONS=${ENABLE_LLM_EXPLANATIONS}

      - OMP_NUM_THREADS=4
      - TZ=${TZ:-UTC}
      # Fix for ONNXRuntime missing libnvrtc.so and libcudnn:
      - LD_LIBRARY_PATH=/opt/conda/lib/python3.10/site-packages/torch/lib:$LD_LIBRARY_PATH
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - app-network

  # 3b. AI Worker - Light (Posture, Nails, Teeth, Oral)
  worker-light:
    image: diseaseprediction-backend:latest
    restart: no
    depends_on:
      redis:
        condition: service_started
      backend:
        condition: service_started
    command: celery -A web_app.backend.celery_app worker --loglevel=info --concurrency=4 -Q q_lightweight
    volumes:
      - ./saved_models:/app/saved_models
      - ./saved_models_onnx:/app/saved_models_onnx # ONNX models
      - ./external:/app/external
    environment:
      - REDIS_URL=redis://redis:6379/0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - ENABLE_LLM_EXPLANATIONS=${ENABLE_LLM_EXPLANATIONS}

      - OMP_NUM_THREADS=4
      - TZ=${TZ:-UTC}
      # Fix for ONNXRuntime missing libnvrtc.so and libcudnn:
      - LD_LIBRARY_PATH=/opt/conda/lib/python3.10/site-packages/torch/lib:$LD_LIBRARY_PATH
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - app-network

  # 4. Monitoring (Flower)
  flower:
    image: diseaseprediction-backend:latest
    command: celery -A web_app.backend.celery_app flower --port=5555 --basic_auth=admin:${FLOWER_PASSWORD:-changeme123}
    restart: no
    ports:
      - "5555:5555"
    environment:
      - REDIS_URL=redis://redis:6379/0
      - FLOWER_BASIC_AUTH=admin:${FLOWER_PASSWORD:-changeme123}
      - TZ=${TZ:-UTC}
    depends_on:
      - redis
    networks:
      - app-network

  # 5. GPU Monitor (Sidecar)
  gpu-monitor:
    image: diseaseprediction-backend:latest
    command: python web_app/backend/gpu_monitor.py
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - TZ=${TZ:-UTC}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - app-network
    profiles:
      - monitor
    restart: no

  # 6. Frontend
  frontend:
    build:
      context: .
      dockerfile: web_app/frontend/Dockerfile
    ports:
      - "5173:80" # Map host 5173 to container 80 (Nginx)
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - app-network

  # 7. Prometheus Metrics
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - app-network

  # 8. Grafana Dashboard
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    networks:
      - app-network

  # 9. Celery Exporter
  celery-exporter:
    image: danihodovic/celery-exporter
    command: [ "--broker", "redis://redis:6379/0" ]
    ports:
      - "9808:9808"
    depends_on:
      - redis
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  redis_data:
  hf_cache:
