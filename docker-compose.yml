services:
  # 1. Message Broker
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    networks:
      - app-network

  # 2. Backend API
  backend:
    build:
      context: .
      dockerfile: web_app/backend/Dockerfile
    ports:
      - "8000:8000"
    command: uvicorn web_app.backend.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - ./saved_models:/app/saved_models # Mount models (Hot-load + No Copy)
      - ./web_app/backend:/app/web_app/backend # Dev code sync
      - ./segformer_utils.py:/app/segformer_utils.py # Mount root files
      - ./inference_pytorch.py:/app/inference_pytorch.py
      - ./inference_new_models.py:/app/inference_new_models.py
      - ./vis_utils.py:/app/vis_utils.py
    environment:
      - REDIS_URL=redis://redis:6379/0
      # Security: Allow frontend access
      - ALLOWED_ORIGINS=http://localhost:5173
      # GPU Support Enabled
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - TZ=${TZ:-UTC}
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]

      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    depends_on:
      - redis
    networks:
      - app-network

  # 3. AI Worker
  worker:
    build:
      context: .
      dockerfile: web_app/backend/Dockerfile
    command: celery -A web_app.backend.celery_app worker --loglevel=info --concurrency=1
    volumes:
      - ./saved_models:/app/saved_models
      - ./segformer_utils.py:/app/segformer_utils.py
      - ./inference_pytorch.py:/app/inference_pytorch.py
      - ./inference_new_models.py:/app/inference_new_models.py
      - ./vis_utils.py:/app/vis_utils.py
      - hf_cache:/root/.cache/huggingface
    environment:
      - REDIS_URL=redis://redis:6379/0
      # GPU Support Enabled
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OMP_NUM_THREADS=4
      - TZ=${TZ:-UTC}
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    depends_on:
      - redis
    networks:
      - app-network

  # 4. Monitoring (Flower)
  flower:
    build:
      context: .
      dockerfile: web_app/backend/Dockerfile
    command: celery -A web_app.backend.celery_app flower --port=5555 --basic_auth=admin:${FLOWER_PASSWORD:-changeme123}
    ports:
      - "5555:5555"
    environment:
      - REDIS_URL=redis://redis:6379/0
      - FLOWER_BASIC_AUTH=admin:${FLOWER_PASSWORD:-changeme123}
      - TZ=${TZ:-UTC}
    depends_on:
      - redis
    networks:
      - app-network

  # 5. GPU Monitor (Sidecar)
  gpu-monitor:
    build:
      context: .
      dockerfile: web_app/backend/Dockerfile
    command: python web_app/backend/gpu_monitor.py
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - TZ=${TZ:-UTC}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - app-network
    restart: unless-stopped

  # 6. Frontend
  frontend:
    build:
      context: .
      dockerfile: web_app/frontend/Dockerfile
    ports:
      - "5173:80" # Map host 5173 to container 80 (Nginx)
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  redis_data:
  hf_cache:
