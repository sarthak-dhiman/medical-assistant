import json
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

class PostureClassifier(nn.Module):
    def __init__(self, input_size=24, num_classes=2): # 12 points * 2 (x,y) = 24 (padded from manual annotations)
        super(PostureClassifier, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, num_classes)
        )
    
    def forward(self, x):
        return self.network(x)

class PostureDataset(Dataset):
    def __init__(self, json_path):
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        self.samples = []
        self.labels = []
        
        FEATURE_SIZE = 24  # 12 keypoints * 2 (x,y)
        for item in data:
            keypoints = item.get('keypoints', [])
            # keypoints is already a flat list of 24 floats (preprocess_mmpose.py guarantees this)
            flat_kps = list(keypoints) if isinstance(keypoints, (list, tuple)) else []

            # Safety: pad or truncate to FEATURE_SIZE
            if len(flat_kps) < FEATURE_SIZE:
                flat_kps += [0.0] * (FEATURE_SIZE - len(flat_kps))
            else:
                flat_kps = flat_kps[:FEATURE_SIZE]
            
            self.samples.append(flat_kps)
            self.labels.append(item.get('label', 0))
        
        self.samples = torch.tensor(self.samples, dtype=torch.float32)
        self.labels = torch.tensor(self.labels, dtype=torch.long)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.samples[idx], self.labels[idx]

def train_model(json_path, save_path=r"D:\Disease Prediction\saved_models\posture_classifier_mmpose.pth"):
    print(f"Loading data from {json_path}...")
    dataset = PostureDataset(json_path)
    
    if len(dataset) == 0:
        print("Error: Dataset is empty.")
        return

    # Split
    train_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)
    train_loader = DataLoader(dataset, batch_size=16, sampler=torch.utils.data.SubsetRandomSampler(train_idx))
    val_loader = DataLoader(dataset, batch_size=16, sampler=torch.utils.data.SubsetRandomSampler(val_idx))
    
    model = PostureClassifier(input_size=24)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    epochs = 50
    print(f"Starting training for {epochs} epochs...")
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        if (epoch + 1) % 10 == 0:
            model.eval()
            correct = 0
            with torch.no_grad():
                for bx, by in val_loader:
                    out = model(bx)
                    _, pred = torch.max(out.data, 1)
                    correct += (pred == by).sum().item()
            
            val_acc = 0
            if len(val_idx) > 0:
                val_acc = 100 * correct / len(val_idx)
                
            print(f"Epoch [{epoch+1}/{epochs}], Loss: {train_loss/len(train_loader):.4f}, Val Acc: {val_acc:.2f}%")

    torch.save(model.state_dict(), save_path)
    print(f"Model saved to {save_path}")

if __name__ == "__main__":
    # Path to the JSON generated by preprocess_mmpose.py
    JSON_PATH = r"D:\Disease Prediction\Dataset\Posture\posture_dataset_mmpose.json"
    if os.path.exists(JSON_PATH):
        train_model(JSON_PATH)
    else:
        print(f"Dataset file {JSON_PATH} not found. Please run 'python scripts/preprocess_mmpose.py' first.")
